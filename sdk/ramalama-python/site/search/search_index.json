{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ramalama Python SDK API documentation generated from docstrings with mkdocstrings.","title":"Home"},{"location":"#ramalama-python-sdk","text":"API documentation generated from docstrings with mkdocstrings.","title":"Ramalama Python SDK"},{"location":"api/","text":"API Reference Ramalama Python SDK public interface for starting a model server and chatting. AsyncRamalamaModel ( model : str , base_image : str | None = None , temp : float | None = None , ngl : int | None = None , max_tokens : int | None = None , threads : int | None = None , ctx_size : int | None = None , timeout : int = 30 ) Bases: RamalamaModelBase Asyncio-friendly Ramalama model interface. __aenter__ () async Async context manager entry that starts the server. __aexit__ ( exc_type , exc_val , exc_tb ) async Async context manager exit that stops the server. chat ( message : str , history : list [ ChatMessage ] | None = None ) -> ChatMessage async Send a chat completion request. Parameters: Name Type Description Default message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running. cleanup () Stop the server process and clean up any container state. download () -> bool async Ensure the model is downloaded locally. Returns: Type Description bool True once the model is available locally. serve () async Start the server and wait until it reports healthy. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. RamalamaServerTimeoutError If the server is not healthy before timeout. start_server () Start the model server in the background. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. stop () async Stop the server and release resources. ChatMessage Bases: TypedDict Chat completion message payload. Attributes: Name Type Description role Literal ['system', 'user', 'assistant', 'developer'] Message author role. content str Message text content. ModelArgs ( temp : float | None = None , ngl : int | None = None , max_tokens : int | None = None , threads : int | None = None , ctx_size : int | None = None ) dataclass Optional model runtime overrides for inference. RamalamaModel ( model : str , base_image : str | None = None , temp : float | None = None , ngl : int | None = None , max_tokens : int | None = None , threads : int | None = None , ctx_size : int | None = None , timeout : int = 30 ) Bases: RamalamaModelBase Synchronous Ramalama model interface. __enter__ () Context manager entry that starts the server. __exit__ ( exc_type , exc_val , exc_tb ) Context manager exit that stops the server. chat ( message : str , history : list [ ChatMessage ] | None = None ) -> ChatMessage Send a chat completion request. Parameters: Name Type Description Default message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running. cleanup () Stop the server process and clean up any container state. download () -> bool Ensure the model is downloaded locally. Returns: Type Description bool True once the model is available locally. serve () Start the server and wait until it reports healthy. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. RamalamaServerTimeoutError If the server is not healthy before timeout. start_server () Start the model server in the background. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. stop () Stop the server and release resources. RamalamaModelBase ( model : str , base_image : str | None = None , temp : float | None = None , ngl : int | None = None , max_tokens : int | None = None , threads : int | None = None , ctx_size : int | None = None , timeout : int = 30 ) Bases: ABC Base class for Ramalama model sessions. Initialize a model session. Parameters: Name Type Description Default model str Model name or identifier. required base_image str | None Container image to use for serving, if different from config. None temp float | None Temperature override for sampling. None ngl int | None GPU layers override. None max_tokens int | None Maximum tokens for completions. None threads int | None CPU threads override. None ctx_size int | None Context window override. None timeout int Seconds to wait for server readiness. 30 args cached property Create and memoize CLI arguments used to start the server. RamalamaNoContainerManagerError Bases: Exception Raised when no supported container manager (docker/podman) is available. RamalamaServerTimeoutError Bases: Exception Raised when the model server fails to become healthy in time. ServerAttributes ( url : str | None = None , open : bool = False ) dataclass Track server address and running state. start ( port : int | str ) Mark the server as running and set its base API URL. stop () Mark the server as stopped and clear its base API URL. is_server_healthy ( port : int | str ) -> bool Check whether the local server health endpoint is responding. Parameters: Name Type Description Default port int | str Port for the local server. required Returns: Type Description bool True if the server responds with HTTP 200. make_chat_request ( model : RamalamaModelBase , message : str , history : list [ ChatMessage ] | None = None ) -> ChatMessage Send a synchronous chat completion request to the running server. Parameters: Name Type Description Default model RamalamaModelBase Active model instance with a running server. required message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running.","title":"API Reference"},{"location":"api/#api-reference","text":"Ramalama Python SDK public interface for starting a model server and chatting.","title":"API Reference"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel","text":"Bases: RamalamaModelBase Asyncio-friendly Ramalama model interface.","title":"AsyncRamalamaModel"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.__aenter__","text":"Async context manager entry that starts the server.","title":"__aenter__"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.__aexit__","text":"Async context manager exit that stops the server.","title":"__aexit__"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.chat","text":"Send a chat completion request. Parameters: Name Type Description Default message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running.","title":"chat"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.cleanup","text":"Stop the server process and clean up any container state.","title":"cleanup"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.download","text":"Ensure the model is downloaded locally. Returns: Type Description bool True once the model is available locally.","title":"download"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.serve","text":"Start the server and wait until it reports healthy. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. RamalamaServerTimeoutError If the server is not healthy before timeout.","title":"serve"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.start_server","text":"Start the model server in the background. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available.","title":"start_server"},{"location":"api/#ramalama_sdk.main.AsyncRamalamaModel.stop","text":"Stop the server and release resources.","title":"stop"},{"location":"api/#ramalama_sdk.main.ChatMessage","text":"Bases: TypedDict Chat completion message payload. Attributes: Name Type Description role Literal ['system', 'user', 'assistant', 'developer'] Message author role. content str Message text content.","title":"ChatMessage"},{"location":"api/#ramalama_sdk.main.ModelArgs","text":"Optional model runtime overrides for inference.","title":"ModelArgs"},{"location":"api/#ramalama_sdk.main.RamalamaModel","text":"Bases: RamalamaModelBase Synchronous Ramalama model interface.","title":"RamalamaModel"},{"location":"api/#ramalama_sdk.main.RamalamaModel.__enter__","text":"Context manager entry that starts the server.","title":"__enter__"},{"location":"api/#ramalama_sdk.main.RamalamaModel.__exit__","text":"Context manager exit that stops the server.","title":"__exit__"},{"location":"api/#ramalama_sdk.main.RamalamaModel.chat","text":"Send a chat completion request. Parameters: Name Type Description Default message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running.","title":"chat"},{"location":"api/#ramalama_sdk.main.RamalamaModel.cleanup","text":"Stop the server process and clean up any container state.","title":"cleanup"},{"location":"api/#ramalama_sdk.main.RamalamaModel.download","text":"Ensure the model is downloaded locally. Returns: Type Description bool True once the model is available locally.","title":"download"},{"location":"api/#ramalama_sdk.main.RamalamaModel.serve","text":"Start the server and wait until it reports healthy. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available. RamalamaServerTimeoutError If the server is not healthy before timeout.","title":"serve"},{"location":"api/#ramalama_sdk.main.RamalamaModel.start_server","text":"Start the model server in the background. Raises: Type Description RamalamaNoContainerManagerError If no container manager is available.","title":"start_server"},{"location":"api/#ramalama_sdk.main.RamalamaModel.stop","text":"Stop the server and release resources.","title":"stop"},{"location":"api/#ramalama_sdk.main.RamalamaModelBase","text":"Bases: ABC Base class for Ramalama model sessions. Initialize a model session. Parameters: Name Type Description Default model str Model name or identifier. required base_image str | None Container image to use for serving, if different from config. None temp float | None Temperature override for sampling. None ngl int | None GPU layers override. None max_tokens int | None Maximum tokens for completions. None threads int | None CPU threads override. None ctx_size int | None Context window override. None timeout int Seconds to wait for server readiness. 30","title":"RamalamaModelBase"},{"location":"api/#ramalama_sdk.main.RamalamaModelBase.args","text":"Create and memoize CLI arguments used to start the server.","title":"args"},{"location":"api/#ramalama_sdk.main.RamalamaNoContainerManagerError","text":"Bases: Exception Raised when no supported container manager (docker/podman) is available.","title":"RamalamaNoContainerManagerError"},{"location":"api/#ramalama_sdk.main.RamalamaServerTimeoutError","text":"Bases: Exception Raised when the model server fails to become healthy in time.","title":"RamalamaServerTimeoutError"},{"location":"api/#ramalama_sdk.main.ServerAttributes","text":"Track server address and running state.","title":"ServerAttributes"},{"location":"api/#ramalama_sdk.main.ServerAttributes.start","text":"Mark the server as running and set its base API URL.","title":"start"},{"location":"api/#ramalama_sdk.main.ServerAttributes.stop","text":"Mark the server as stopped and clear its base API URL.","title":"stop"},{"location":"api/#ramalama_sdk.main.is_server_healthy","text":"Check whether the local server health endpoint is responding. Parameters: Name Type Description Default port int | str Port for the local server. required Returns: Type Description bool True if the server responds with HTTP 200.","title":"is_server_healthy"},{"location":"api/#ramalama_sdk.main.make_chat_request","text":"Send a synchronous chat completion request to the running server. Parameters: Name Type Description Default model RamalamaModelBase Active model instance with a running server. required message str User prompt content. required history list [ ChatMessage ] | None Optional prior conversation messages. None Returns: Type Description ChatMessage Assistant message payload. Raises: Type Description RuntimeError If the server is not running.","title":"make_chat_request"}]}